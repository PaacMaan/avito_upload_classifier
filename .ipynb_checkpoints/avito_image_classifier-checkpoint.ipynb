{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"fluid-row\" id=\"section-header\">\n",
    "    <h2 class=\"title toc-ignore\">Master Data Science & Big Data - ENSIAS</h2>\n",
    "    <h4 class=\"author\"><em>pacman</em></h4>\n",
    "    <h4 class=\"date\"><em>Mai 17, 2018</em></h4>\n",
    "</div>\n",
    "<center>\n",
    "    <h1>\n",
    "        <u>Deep Learning in Computer Vision</u>\n",
    "    </h1>\n",
    "    <h4>Realized by : Ayoub RMIDI</h4>\n",
    "</center>\n",
    "<div class=\"fluid-row\" id=\"section-header\">\n",
    "    <h2 class=\"title toc-ignore\">Introduction</h2>\n",
    "    <p class=\"lead\">The business problem I will try to solve, is classifying images of published ads from Avito website, personally many times, by mistake when I am trying to upload pictures to sell a phone for example, I found my self putting it into laptops category which is not the case, so the idea is to adopt a Deep Learning model that can classify the uploaded images in the right class in real time based on Avito's data set that we will scrap to train our model.<br>\n",
    "In this notebook I will cover only 2 classes which are <b>phone</b> and <b>laptops</b> for simplicity, hence we can improve our model later by adding multiple classes. The following notebook would be presented as follow :\n",
    "    <ol class=\"lead\">\n",
    "        <li>Data Collection</li>\n",
    "        <li>Data Preprocessing</li>\n",
    "        <li>Data Exploration</li>\n",
    "        <li>Data Modeling</li>\n",
    "        <li>Model Deployment & Evaluation</li>\n",
    "    </ol>\n",
    "<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"lead\">The data we will work on is a set of images scrapped from Avito.ma for each category laptops and moto, the resulted folder will contain two sub directories called respectively \"laptop\" and \"phone\" and the size of downloaded images is 120x90, with 3 channels RGB for each one. Here is the following code doing the data scrapping.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we import needed libraries\n",
    "import tensorflow as tf\n",
    "from skimage import data\n",
    "import matplotlib.pyplot as plt\n",
    "import os, random, time, shutil, requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimage import transform\n",
    "from matplotlib import pyplot\n",
    "from lxml import html\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import Dense\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_image(ad_name, file_name, item_link, download_path):\n",
    "    response = requests.Session().get(item_link, stream=True)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(os.path.join(download_path, file_name), 'wb') as image_file:\n",
    "            for chunk in response.iter_content(1024):\n",
    "                image_file.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_images(url, path):\n",
    "    response = requests.Session().get(url).text\n",
    "    #print(response)\n",
    "    tree = html.fromstring(response)\n",
    "    for ad in tree.xpath('//div[contains(@class, \"item\") and contains(@class, \"li-hover\")]'):\n",
    "        if ad.xpath('.//img/@data-original'):\n",
    "            title = ad.findtext('.//h2/a')\n",
    "            image_url = ad.xpath('.//img/@data-original')[-1]\n",
    "            image_name = image_url.split('/')[-1]\n",
    "            save_image(title, image_name, image_url, path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data scrapping\n",
      "page  60 attended\n",
      "End scrapping images\n"
     ]
    }
   ],
   "source": [
    "basic_url = \"https://www.avito.ma/fr/casablanca/ordinateurs_portables-Ã _vendre?o=\"\n",
    "path = 'raw_data/laptop'\n",
    "print('Starting data scrapping')\n",
    "for i in range(0,70):\n",
    "    page_url = basic_url + str(i)\n",
    "    scrape_images(page_url, path=path)\n",
    "    if i%10 == 0:\n",
    "        print('page ',i,'attended')\n",
    "print('End scrapping images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are exactly 2097 images on laptop class.\n"
     ]
    }
   ],
   "source": [
    "number_of_laptop_images = len(os.listdir('raw_data/laptop'))\n",
    "print('there are exactly',number_of_laptop_images,'images on laptop class.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are exactly 2180 images on phone class.\n"
     ]
    }
   ],
   "source": [
    "number_of_phone_images = len(os.listdir('raw_data/phone'))\n",
    "print('there are exactly',number_of_phone_images,'images on phone class.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='lead'>At the end we will have a folder containing images of laptops which looks like this :</p>\n",
    "<img src=\"notebook_pics/laptops.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='lead'>For the preprocessing task we will divide into two subtasks as follow :</p>\n",
    "<ol class='lead'>\n",
    "    <li>Remove noizy Data</li>\n",
    "    <li>Image resizing</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Removing Noisy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='lead'>To remove noisy data, we have to check every image in the data set and delete those who will not help us to get a good accuracy, as an example we can find in the phone directory an image like this :</p>\n",
    "<img src='notebook_pics/8000532171.jpg' width='120' height='90'>\n",
    "<p class='lead'>which represent a phone's packet and not a real phone as we are lookig for, so I delete it.</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Image resizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"lead\">To fit the LeNet-5 [1] Model Architecture, we will have to resize images from 120x90 to 32x32 for each class, So we create a new folder where we will store our resulted data set by creating 2 subfolders called 0 and 1 which refer to <b>laptop</b>,  and <b>phone</b> respectively. The next section show a Snapshot of the code doing this task :</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def resize_and_save(target_path, cible_path, dirs):\n",
    "    for item in dirs:\n",
    "        im = Image.open(cible_path+item)\n",
    "        file, extension = os.path.splitext(cible_path+item)\n",
    "        imResized = im.resize((64, 64), Image.ANTIALIAS)\n",
    "        filename = file.split('/')[2]\n",
    "        imResized.save(target_path + filename + extension, 'JPEG', quality=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='lead'>Now we call our built-in function for each target and cible directory.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting resizing images ...\n",
      "End images resizing.\n"
     ]
    }
   ],
   "source": [
    "# target_path list\n",
    "target_path = ['preprocessed_data/phone/', 'preprocessed_data/laptop/']\n",
    "original_path = ['raw_data/phone/', 'raw_data/laptop/']\n",
    "print('Starting resizing images ...')\n",
    "for t, c in zip(target_path, original_path):\n",
    "    dirs = os.listdir(c)\n",
    "    resize_and_save(t, c, dirs)\n",
    "    #print(t, c)\n",
    "print('End images resizing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"table table-bordered\">\n",
    "    <thead>\n",
    "      <tr>\n",
    "        <th>Before resizing (120x90)</th>\n",
    "        <th>After resizing (64x64)</th>\n",
    "      </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "      <tr>\n",
    "        <td><img src=\"raw_data/laptop/0005474607.jpg\"></td>\n",
    "        <td><img src=\"preprocessed_data/laptop/0005474607.jpg\"></td>\n",
    "      </tr>\n",
    "    </tbody>\n",
    "  </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Data Exploration</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='lead'>First of all we start by a simple histogram showing the distribution of images per class, to get an idea about the balancing of our data set.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['phone', 'laptop']\n",
    "values = [number_of_phone_images, number_of_laptop_images]\n",
    "ypos = np.arange(len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(9, 8))\n",
    "plt.xticks(ypos, classes)\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Images distribution over classes')\n",
    "plt.bar(ypos, values, color=['blue','orange'], width=0.75)\n",
    "plt.savefig('temp.png')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='lead'>As we can see from the plot above, the two classes are quite balanced, which is a mandatory step to build a high accurate Deep Learning Model, so that we won't get a class overriding another one.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='lead'>After exploring the data set, we will start splitting it into training and test set. To perform this task we will split the preprocessed data set into 80% training and 20% for test, for each class.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we define how many items do we have per class\n",
    "class_size = [number_of_phone_images, number_of_laptop_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def move_training_files(which_class, origin_path, destination, ratio):\n",
    "    training_size = int(ratio * which_class)\n",
    "    files = os.listdir(origin_path)\n",
    "    training_data = files[:training_size]\n",
    "    for file in training_data:\n",
    "        shutil.move(origin_path + file, destination + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we define lists of destination and target pathtu m\n",
    "destination_train_path = ['data/train/phone/', 'data/train/laptop/']\n",
    "destination_test_path = ['data/validation/phone/', 'data/validation/laptop/']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copying Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start copying files to train folder ...\n",
      "End copying files.\n"
     ]
    }
   ],
   "source": [
    "print('Start copying files to train folder ...')\n",
    "for c, o, d in zip(class_size, target_path, destination_train_path):\n",
    "    move_training_files(c, o, d, 0.8)\n",
    "print('End copying files.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copying Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start copying files to train folder ...\n",
      "End copying files.\n"
     ]
    }
   ],
   "source": [
    "print('Start copying files to train folder ...')\n",
    "for c, o, d in zip(class_size, target_path, destination_test_path):\n",
    "    move_training_files(c, o, d, 0.2)\n",
    "print('End copying files.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Modeling - Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten\n",
    "# Initialise CNN\n",
    "model = Sequential()\n",
    "\n",
    "# Add convolution layer\n",
    "model.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "\n",
    "# Add max pooling layer\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Add a second convolution layer\n",
    "model.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "\n",
    "# Add max pooling layer\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Flattening the output\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add full connection\n",
    "model.add(Dense(units = 128, activation = 'relu'))\n",
    "model.add(Dense(units = 2, activation = 'sigmoid'))\n",
    " \n",
    "# Compiling the CNN\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 29, 29, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 813,346\n",
      "Trainable params: 813,346\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images and Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='lead'>To feed images to our compiled model, we will have to `ImageDataGenerator` function, this will help us generate batches of tensor image data with real-time data augmentation. The data will be looped over (in batches).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create an instance for training_data\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# Create an instance for testing_data\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='lead'>Now that we have created, two instances of `ImageDataGenerator`, we need to feed them with the right path to both training and validation dataset, with a `categorical` class mode.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3421 images belonging to 2 classes.\n",
      "Found 855 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_set = train_datagen.flow_from_directory(\n",
    "        'data/train',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=64,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_set = validation_datagen.flow_from_directory(\n",
    "        'data/validation',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=64,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='lead'>The last but not the least step is to make our built-in model fit the dataset, for this reason we feed it with the `train_set` and `validation_set` already created, and start the training session.<br>But before that we will set a callback to visualize the training evolution using TensorBoard.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./tf-log', histogram_freq=0,\n",
    "                         write_graph=True, write_images=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "100/100 [==============================] - 107s 1s/step - loss: 0.3987 - acc: 0.8140 - val_loss: 0.4216 - val_acc: 0.7982\n",
      "Epoch 2/25\n",
      "100/100 [==============================] - 108s 1s/step - loss: 0.3680 - acc: 0.8279 - val_loss: 0.4074 - val_acc: 0.8036\n",
      "Epoch 3/25\n",
      "100/100 [==============================] - 100s 1s/step - loss: 0.3404 - acc: 0.8417 - val_loss: 0.4365 - val_acc: 0.7923\n",
      "Epoch 4/25\n",
      "100/100 [==============================] - 102s 1s/step - loss: 0.3451 - acc: 0.8429 - val_loss: 0.3840 - val_acc: 0.8187\n",
      "Epoch 5/25\n",
      "100/100 [==============================] - 101s 1s/step - loss: 0.3320 - acc: 0.8524 - val_loss: 0.4020 - val_acc: 0.8182\n",
      "Epoch 6/25\n",
      "100/100 [==============================] - 102s 1s/step - loss: 0.3158 - acc: 0.8646 - val_loss: 0.3401 - val_acc: 0.8409\n",
      "Epoch 7/25\n",
      "100/100 [==============================] - 100s 1s/step - loss: 0.2881 - acc: 0.8705 - val_loss: 0.3499 - val_acc: 0.8579\n",
      "Epoch 8/25\n",
      "100/100 [==============================] - 105s 1s/step - loss: 0.2772 - acc: 0.8829 - val_loss: 0.4059 - val_acc: 0.8128\n",
      "Epoch 9/25\n",
      "100/100 [==============================] - 103s 1s/step - loss: 0.2752 - acc: 0.8764 - val_loss: 0.4128 - val_acc: 0.8112\n",
      "Epoch 10/25\n",
      "100/100 [==============================] - 105s 1s/step - loss: 0.2776 - acc: 0.8838 - val_loss: 0.3472 - val_acc: 0.8451\n",
      "Epoch 11/25\n",
      "100/100 [==============================] - 101s 1s/step - loss: 0.2543 - acc: 0.8953 - val_loss: 0.3871 - val_acc: 0.8182\n",
      "Epoch 12/25\n",
      "100/100 [==============================] - 104s 1s/step - loss: 0.2297 - acc: 0.9057 - val_loss: 0.3444 - val_acc: 0.8620\n",
      "Epoch 13/25\n",
      "100/100 [==============================] - 103s 1s/step - loss: 0.2267 - acc: 0.9035 - val_loss: 0.3017 - val_acc: 0.8760\n",
      "Epoch 14/25\n",
      "100/100 [==============================] - 107s 1s/step - loss: 0.2149 - acc: 0.9092 - val_loss: 0.3194 - val_acc: 0.8684\n",
      "Epoch 15/25\n",
      "100/100 [==============================] - 108s 1s/step - loss: 0.1993 - acc: 0.9208 - val_loss: 0.3295 - val_acc: 0.8579\n",
      "Epoch 16/25\n",
      "100/100 [==============================] - 104s 1s/step - loss: 0.2080 - acc: 0.9157 - val_loss: 0.3035 - val_acc: 0.8726\n",
      "Epoch 17/25\n",
      "100/100 [==============================] - 103s 1s/step - loss: 0.1899 - acc: 0.9270 - val_loss: 0.3184 - val_acc: 0.8608\n",
      "Epoch 18/25\n",
      "100/100 [==============================] - 103s 1s/step - loss: 0.1763 - acc: 0.9255 - val_loss: 0.3059 - val_acc: 0.8836\n",
      "Epoch 19/25\n",
      "100/100 [==============================] - 101s 1s/step - loss: 0.1800 - acc: 0.9275 - val_loss: 0.3126 - val_acc: 0.8790\n",
      "Epoch 20/25\n",
      "100/100 [==============================] - 100s 996ms/step - loss: 0.1604 - acc: 0.9374 - val_loss: 0.3327 - val_acc: 0.8702\n",
      "Epoch 21/25\n",
      "100/100 [==============================] - 99s 992ms/step - loss: 0.1331 - acc: 0.9468 - val_loss: 0.4014 - val_acc: 0.8778\n",
      "Epoch 22/25\n",
      "100/100 [==============================] - 99s 985ms/step - loss: 0.1548 - acc: 0.9343 - val_loss: 0.3240 - val_acc: 0.8761\n",
      "Epoch 23/25\n",
      "100/100 [==============================] - 100s 1s/step - loss: 0.1366 - acc: 0.9456 - val_loss: 0.3535 - val_acc: 0.8743\n",
      "Epoch 24/25\n",
      "100/100 [==============================] - 99s 991ms/step - loss: 0.1271 - acc: 0.9517 - val_loss: 0.4390 - val_acc: 0.8533\n",
      "Epoch 25/25\n",
      "100/100 [==============================] - 101s 1s/step - loss: 0.1313 - acc: 0.9477 - val_loss: 0.3522 - val_acc: 0.8772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f734c713a20>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_set,\n",
    "        steps_per_epoch=100,\n",
    "        epochs=25,\n",
    "        validation_data=validation_set,\n",
    "        validation_steps=2000, callbacks = [TrainValTensorBoard(write_graph=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=lead>Usually we build a test set apart from the validation where we evaluate our final trained model, but we will keep it simple and validate it only on the same validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3527091883856247, 0.8772840283984867]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_sample_size = 800\n",
    "model.evaluate_generator(validation_set, validation_sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='lead'>As result we got a validation accuracy of 83.27% with a loss value of 0.37 which is kind of critical a bit, since it's still need to decrease.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models weights on hdf5 extension\n",
    "models_dir = 'models/'\n",
    "model.save(models_dir+'avito_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(models_dir+'avito_model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model on some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array\n",
    "from PIL import Image\n",
    "def preprocess_image(image,target_size):\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "    image = image.resize(target_size)\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image,axis = 0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"5531569558.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=67x90 at 0x7F5BACE9E278>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_image = preprocess_image(img,target_size = (64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = []\n",
    "matrix.append(processed_image)\n",
    "prediction = model.predict(matrix).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 1.0]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone\n"
     ]
    }
   ],
   "source": [
    "if np.asscalar(np.argmax(prediction, axis=1)) == 0:\n",
    "    print('Laptop')\n",
    "else:\n",
    "    print('Phone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
